---
layout: post
title: Slow ping, slow SSH, fast dig, fast nslookup (spoiler check nscd)
comments: true
tags: 
  - fail
  - ubuntu
  - nscd
  - salt
  - strace
---

Yesterday I had an issue where some of the nodes I'm responsible for were responding very slowly to SSH access.  Zabbix was throwing "SSH service down" alerts.  The nodes were responding to ping ok and I could `nc` port 22 without issue.
<!--more-->

#### strange behavior

SSH would eventually connect, but it would take almost a minute.  Once I was shelled in, there were other strange behaviors.  `ls`'ing was slow, tab complete was slow, anything with `sudo` was slow.  What the heck?

Interesting to note (as we'll see later), that these problems correlated with the failure and reboot of another host on our network.  This host is responsible for authoritative DNS for `int.acme.com`, LDAP auth and NFS user directory mounts.

The problem would mysteriously go away on one node only to crop up on another node.  This was especially bizarre.

One of the most interesting data points I had was that once I was logged into a problem node, trying to ping any host in a particular subdomain (`int.acme.com`) resulted in an initial 10 second delay and then the pings were fine.

~~~ sh
user@n534:~$ time ping -c1 -n tank.int.acme.com
PING tank.int.acme.com (10.190.0.20) 56(84) bytes of data.
64 bytes from 10.190.0.20: icmp_seq=1 ttl=63 time=0.117 ms

--- tank.int.acme.com ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.117/0.117/0.117/0.000 ms

real    0m10.013s
user    0m0.002s
sys     0m0.000s
~~~

nslookup was fast:

~~~ sh
user@n534:~$ time nslookup tank.int.acme.com
Server:         10.190.20.7
Address:        10.190.20.7#53

Non-authoritative answer:
Name:   tank.int.acme.com
Address: 10.190.0.20

real    0m0.113s
user    0m0.006s
sys     0m0.006s
~~~

dig was fast:

~~~ sh
talley@n534:~$ time dig tank.int.acme.com

; <<>> DiG 9.9.5-3ubuntu0.8-Ubuntu <<>> tank.int.acme.com
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 53395
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:
;tank.int.acme.com.  IN      A

;; ANSWER SECTION:
tank.int.acme.com. 3 IN      A       10.190.0.20

;; Query time: 0 msec
;; SERVER: 10.190.20.7#53(10.190.20.7)
;; WHEN: Tue May 31 16:01:13 MDT 2016
;; MSG SIZE  rcvd: 62

real    0m0.024s
user    0m0.003s
sys     0m0.006s
~~~

I analyzed a pcap of ping and it looked as I suspected.  There was a 10s delay before the pings are shown the pcap, but there was not any large delay between an echo request and reponse.

Pings to hosts in _other_ subdomains (ex. `lab.acme.com`) had no initial delay.  Interesting!

Googling led to a lot of rabbit holes and red herrings with IPv6 and DNS reverse lookups and misconfigured `/etc/hosts`.  Because the problem started occurruing randomly and not after any system config changes, these options seemed unlikely.

So, what _was_ ping doing for 10 seconds before sending an echo request to the wire?  Let's use strace to try and find out.

#### stracing

Here's the command I used:

~~~ sh
root@n534:~# strace -t -o ping_trace.txt ping -c1 tank.int.acme.com
~~~

(The full trace file is [here](https://gist.github.com/b225ccc/2362cdf98b9d0400e53a6f81d50702fb) and embedded at the end of this post.  I had to redact some info if some of the addressing/etc. might not line up exactly.)

The interesting parts start around line 70:

~~~ sh
[...]
15:48:44 socket(PF_LOCAL, SOCK_STREAM|SOCK_CLOEXEC|SOCK_NONBLOCK, 0) = 4
15:48:44 connect(4, {sa_family=AF_LOCAL, sun_path="/var/run/nscd/socket"}, 110) = 0
15:48:44 sendto(4, "\2\0\0\0\r\0\0\0\6\0\0\0hosts\0", 18, MSG_NOSIGNAL, NULL, 0) = 18
15:48:44 poll([{fd=4, events=POLLIN|POLLERR|POLLHUP}], 1, 5000) = 0 (Timeout)
15:48:49 close(4)                       = 0
15:48:49 socket(PF_LOCAL, SOCK_STREAM|SOCK_CLOEXEC|SOCK_NONBLOCK, 0) = 4
15:48:49 connect(4, {sa_family=AF_LOCAL, sun_path="/var/run/nscd/socket"}, 110) = 0
15:48:49 sendto(4, "\2\0\0\0\4\0\0\0\35\0\0\0tank.int.acme.com"..., 41, MSG_NOSIGNAL, NULL, 0) = 41
15:48:49 poll([{fd=4, events=POLLIN|POLLERR|POLLHUP}], 1, 5000) = 0 (Timeout)
15:48:54 close(4)                       = 0
[...]
~~~

Hm, two `poll`s with 5s timeouts to a local socket at `/var/run/nscd/socket`.  If these connections were failing (or if no response was received), we would observe a 10s delay, so this seems promising.  And, we can see from the timestamps, that there is indeed a 10s delay.

#### nscd implicated

So, now, `nscd` is implicated and is starting to seem like a logical culprit.  `nscd` is caching various pieces of information, and specifically as it relates to this problem: hosts entries and ldap auth info.  What if the nscd database was corrupt or the process/socket wasn't responding correctly?

The theory is easy enough to test; let's stop the nscd process and try the ping again.  Trying to gracefully stop the nscd process was just hanging (more evidence), so I `kill -9`'ed.  Then:

~~~ sh
user@n534:~$ time ping -c1 tank.int.acme.com
PING tank.int.acme.com (10.190.0.20) 56(84) bytes of data.
64 bytes from 10.190.0.20: icmp_seq=1 ttl=63 time=0.087 ms

--- tank.int.acme.com ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.087/0.087/0.087/0.000 ms

real	0m0.006s
user	0m0.000s
sys	    0m0.001s
~~~

Boom.  I also started nscd and tried again.  Results are still good.  So, **that confirms nscd was the problem**.

#### why?

But, why?  I still don't actually know, unfortunately.  I tried to get some insight into the state of nscd while it was in the problem state, but I couldn't get much.  You can use `strings` to get some insight into the nscd db, but it isn't that helpful.  nscd logging is/was disabled by default (Ubuntu 14.04), so no help there.  Also, `nscd -g` (for stats) just hung, so, again, no help there.

My best guess comes back to the host mentioned above that serves some of our DNS and ldap auth.  When this host failed and rebooted, I suspect it put the nscd database on some hosts into a corrupt or odd state.

I'm also not sure why this only seemed to affect some nodes and at random intervals.

#### salt plug

For the devops spin, here's a quick salt and remote execution plug.  The salt remote execution engine makes restarting this service on all hosts pretty trivial:

~~~ sh
root@saltmaster:/srv/salt# salt '*' cmd.run 'killall -9 nscd && service nscd start'
~~~

Note that I used  [`cmd.run`](https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.cmdmod.html#salt.modules.cmdmod.run) instead of [`service.reload`](https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.service.html#module-salt.modules.service) because nscd was hanging on a graceful shutdown.


---

Reference(s):

1. [http://www.thegeekstuff.com/2011/11/strace-examples/](http://www.thegeekstuff.com/2011/11/strace-examples/)
2. [http://forums.fedoraforum.org/showthread.php?t=236568](http://forums.fedoraforum.org/showthread.php?t=236568)
3. [https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=335476](https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=335476)

---

{% gist b225ccc/2362cdf98b9d0400e53a6f81d50702fb %}
